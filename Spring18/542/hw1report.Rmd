---
title: "Homework 01"
sub-title: "ghosh17"
author: "Subhankar Ghosh"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1

```{r, echo=FALSE, comment=FALSE, results='hide', error=FALSE, warning=FALSE}
library(mlbench)
library(ggplot2)
library(reshape2)
library(leaps)
```


```{r, echo=FALSE, comment=FALSE, results='hide', error=FALSE, warning=FALSE}
data("BostonHousing")
```

### (a) Perform a descriptive analysis on all variables. Comment on any potential issues and address them if needed.

**The Columns in the Boston Housing dataset are as follows:**

  * crim	: per capita crime rate by town
  * zn	: proportion of residential land zoned for lots over 25,000 sq.ft
  * indus	: proportion of non-retail business acres per town
  * chas : Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
  * nox	: nitric oxides concentration (parts per 10 million)
  * rm	: average number of rooms per dwelling
  * age	: proportion of owner-occupied units built prior to 1940
  * dis	: weighted distances to five Boston employment centres
  * rad	: index of accessibility to radial highways
  * tax	: full-value property-tax rate per USD 10,000
  * ptratio	: pupil-teacher ratio by town
  * b	: 1000(B - 0.63)^2 where B is the proportion of blacks by town
  * lstat	: percentage of lower status of the population
  * medv : median value of owner-occupied homes in USD 1000's

Let us look at the Min, Mean, Median, Max values of the variables

```{r, echo=FALSE, comment="", results='markup', error=FALSE, warning=FALSE}
s=summary(BostonHousing)
knitr::kable(t(s[c(1,3,4,6),]), caption="Summary of variables")
```

SOme of the variables like **crim** has a very high Maximum value compared to its mean and 3rd quantile so there might be some leverage points due to this.


```{r, echo=FALSE, comment="", results='markup', error=FALSE, warning=FALSE}
corr = round(cor(BostonHousing[,-4]), 2)

get_lower_tri<-function(cormat){
  cormat[upper.tri(cormat)] <- NA
  return(cormat)
}

corrplt <- get_lower_tri(corr)
corrplt = melt(corrplt)
# Correlation plot
corrheatmap = ggplot(data = corrplt, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1,1), space = "Lab", na.value = 'white', 
                       name="Pearson\nCorrelation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                   size = 12, hjust = 1)) +
  coord_fixed()

corrheatmap + geom_text(aes(Var1, Var2, label = value), color = "black", size = 3) +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    axis.ticks = element_blank(),
    legend.justification = c(1, 0),
    legend.position = c(0.6, 0.7),
    legend.direction = "horizontal") +
  ggtitle("Correlation Matrix") + 
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1, title.position = "top", title.hjust = 0.5))
```

  * We can see a very **strong correlation of 0.91** between *rad* and *tax* which makes sense because we would expect full-value property-tax rate to go up as accessibility to radial highways increase.
  * We can see a **high negative correlation of -0.77** between *nox* and *dis* so we can say that the concentration of nitrogen oxide increases near the employment centers.


### (b) Perform the best subset selection using BIC criterion. Report the best model (the selected variables and their parameters).

```{r, echo=FALSE, comment="", results='hide', error=FALSE, warning=FALSE}
n = nrow(BostonHousing)
lmfit = lm(medv ~ ., data = BostonHousing)
best_bic = step(lmfit, direction="both", k=log(n), trace=0)
```

Performing best subset selection using BIC we get the best model as:

```{r, echo=FALSE, comment="", results='markup', error=FALSE, warning=FALSE}
best_bic$call
```

With a BIC value of:

```{r, echo=FALSE, comment="", results='markup', error=FALSE, warning=FALSE}
min(best_bic$anova$AIC)
```

Variables and parameters of best BIC model

```{r, echo=FALSE, comment="", results='markup', error=FALSE, warning=FALSE}
knitr::kable(data.frame(best_bic$coefficients), caption="Variables and parameters of best BIC model")
```


### (c) Perform i) forward stepwise selection using AIC criterion; and ii) backward stepwise selection using Marrow's $C_p$ criterion. Compare these two models with the model in part b).

**(i)** Performing best subset selection using AIC we get the best model as:

```{r, echo=FALSE, comment="", results='markup', error=FALSE, warning=FALSE}
best_aic = step(lmfit, direction="forward", k=2, trace=0)
best_aic$call
```

With an AIC value of 

```{r, echo=FALSE, comment="", results='markup', error=FALSE, warning=FALSE}
best_aic$anova$AIC
```


```{r, echo=FALSE, comment="", results='markup', error=FALSE, warning=FALSE}
knitr::kable(data.frame(best_aic$coefficients), caption="Variables and parameters of best AIC model")
```


**(ii)** Performing best subset selection using $C_p$ we get the best model as:

```{r, echo=FALSE, comment="", results='hide', error=FALSE, warning=FALSE}
bb = BostonHousing
RSSleaps=regsubsets(medv ~ ., data = BostonHousing, nvmax = NULL, method = "backward")
sumleaps = summary(RSSleaps, matrix = T)
sumleaps$outmat
sumleaps$cp
```

lm(formula = medv ~ crim + zn + chas + nox + rm + dis + rad + tax + 
     ptratio + b + lstat, data = BostonHousing)

With a minimum $C_p$ value of:

```{r, echo=FALSE, comment="", results='hide', error=FALSE, warning=FALSE}
min(sumleaps$cp)
```

We observe that the best model for obtained by using $C_p$ and BIC are the same with 11 predictor variables.

But the AIC model includes all 13 predictor variables.

### (d)  Comment on the advantages and disadvantages of the selection algorithms (best subset, forward, backward and stepwise). If you get different results using these three algorithms (assume that you use the same selection criterion), would you prefer some over others? Why?

*Advantages*

*Best-Subset Selection:* If we consider all combinations then it will consider $2^n$ combinations (where n is the number of independent variables) which is computationally infeasible if n is very large. But for small n it goes through all possible subsets of n in an incremental/decreasing fashion so it gets the best subset.

*Forward Selection:* Since it starts from intercept model, it works well for cases where number of predictor variables are more than the observations.
Forward stepwise will have lower variance.


*Backward Selection:* It is computationally less expensive and tends to give better models. 

*Disdvantages*

*Best-Subset Selection:* It can't work for large p(p>40) as it will become computationally infeasible since it goes through all $2^p$ subsets still it becomes computationally infeasible.

*Forward Selection:*  As forward stepwise is a more constrained search, it will have more bias than models chosen by other selections.

*Backward Selection:* Backward selection can only be used when N > p(where N is the number of records and p is the number of variables). Since it starts from full model, it will require atleast n=p, i.e. a full rank matrix to give unique Beta values.


I would prefer Best-Subset Selection Algorithm over all others as it checks for all subsets of the predictor variables.



### (e) Comment on the advantages and disadvantages of the three selection criteria (AIC, BIC, $C_p$). If you get different results using these three criteria (assuming the same algorithm), would you prefer some over others? Why?


AIC works better when the data is best modelled by a nonparametric model and BIC when the data is best modelled by a parametric model. When *n* is large, the costs incured by BIC is a lot more than AIC (or $C_p$). So AIC tends to pick a larger model than BIC. $C_p$ works similar to AIC.

If the model is a parametric model then I would prefer AIC over others and in case of non-parametric model I would prefer BIC over AIC and $C_p$.


# Question 2

### (a)  Provide a short summary of the dataset and the research goal.

**Data Summary:** Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. The training and test data sets have 785 columns. The first column consists of the class labels (see above), and represents the article of clothing.

**Research Goal:** In this exercise the research goal is to do image classification by implementing a fast knn algorithm. This dataset is one of the most standard datasets to do image classification. KNN is a very well know approach to classify based on neighbourhood approach.

### (b) Write your R code for kNN. This is a fairly large dataset, so you should consider writing an efficient algorithm. If it is very slow, you can consider doing part d) first. In addition, how do you deal with ties when k is even.

Function to get the misclassification rate

```{r, eval=FALSE}
## Function to compute the Misclassification rate
## Input variables:
## pred: predicted values
## actual: actual values
get_misclassification <- function(pred, actual) {
    mean(pred != actual)
}
```

Implementation of knn model

```{r, eval=FALSE}
## Function to implement knn
# k : choice of k
# x_test : Matrix of predictor variables of testset
# x_train : Matrix of predictor variables of trainset
# y_test : vector of class of testset
# y_train : vector of class of trainset
# return :: predictions for the x_test data
myknn <- function(k, x_test, x_train, y_test, y_train)
{
  y_pred = rep(0, nrow(x_test))
  for(i in 1:nrow(x_test))
  {
    ## Euclidean distance
    one_dist = sqrt(colSums((t(x_train) - x_test[i,])^2))
    one_df = as.data.frame(one_dist)
    a = y_train[head(sort(one_df$one_dist, index.return = TRUE, decreasing = FALSE)$ix, k)]
    y_pred[i] = a[max(table(a))]
  }
  y_pred
}
```


In case of ties we can select the best class randomly from the tied classes.


### (c)  Fit your kNN model to the training data, and predict the labels using the testing data. Tune the parameter k to obtain the best testing error and comment on the effect of k. Summarize the performance of the final model. What is the degrees of freedom?

```{r, echo=FALSE, comment="", results='hide', error=FALSE, warning=FALSE}
res = read.csv("result.csv")
```

```{r, echo=FALSE, comment="", results='asis', error=FALSE, warning=FALSE}
ggplot(data = res, aes(x=k, y=Accuracy)) +
  geom_line() + geom_point() + 
  theme_bw() + ggtitle("Test Accuracy vs k plot")
```




```{r, echo=FALSE, comment="", results=='asis', error=FALSE, warning=FALSE}
res$X = NULL
res$p = NULL
res$k = as.character(res$k)
knitr::kable(t(res), align = 'c', caption = "Performance summary of knn as k varies")
```


From the plot of Test Accuracy vs k we can see that the highest accuracy of 0.8457 at k=4. 
Since the degree of freedom of knn is given by $N/k$ the degree of freedom of the best model is 15000.


### (d)  Can you suggest some approaches that can speed up the computation (even at some minor cost of prediction accuracy)?

For speeding up the computation we can do the following:
  
  * Instead of using a loop to iterate over the all the predictor variables to calculate the euclidean distance we can sweep through the predictor variables in R so that the computation for all the predictor variables can happen parallely.
  * Since we have a large number of predictor variables we can apply PCA to reduce dimension of the dataset and speed up the computation.




